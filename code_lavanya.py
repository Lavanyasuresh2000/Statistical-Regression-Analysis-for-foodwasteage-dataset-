# -*- coding: utf-8 -*-
"""code_lavanya.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1n8AXX2REYdNH7O6BoqAnzSXnXZht5Q0Y

##DATA PREPARATION
"""

from pandas import  read_csv, DataFrame,Series, get_dummies
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import GridSearchCV
#from sklearn import linear_model
from sklearn.linear_model import SGDRegressor
from plotly import graph_objs, figure_factory
import joblib
import pandas as pd

# reading and metadata
data1 = read_csv("/content/food_wastage_data.csv")
data1.head()
data1.shape
print(data1.info())

data1['Type of Food'] = data1['Type of Food'].map({'Meat':0, 'Baked Goods':1, 'Dairy Products':2, 'Vegetables':3, 'Fruits':4})
data1['Event Type'] = data1['Event Type'].map({'Corporate':0, 'Birthday':1, 'Wedding':2, 'Social Gathering':3})
data1['Purchase History'] = data1['Purchase History'].map({'Regular':0, 'Occasional':1})
data1['Seasonality'] = data1['Seasonality'].map({'All Seasons':0, 'Winter':1,'Summer':2})
data1['Preparation Method'] = data1['Preparation Method'].map({'Buffet':0, 'Finger Food':1,'Sit-down Dinner':2})
data1['Geographical Location'] = data1['Geographical Location'].map({'Urban':0, 'Suburban':1,'Rural':2})
data1['Pricing'] = data1['Pricing'].map({'Low':0, 'Moderate':1,'High':2})
data1['Storage Conditions'] = data1['Storage Conditions'].map({'Refrigerated':0, 'Room Temperature':1})

data1.info()

#Heatmap
correlation = data1.corr()
# print (correlation)
f = figure_factory.create_annotated_heatmap(correlation.values,list(correlation.columns),list(correlation.columns),correlation.round(2).values,showscale=True)
f.show()

# Dividing dataset into label and feature sets
X = data1.drop(['Wastage Food Amount'], axis = 1) # Features
Y = data1['Wastage Food Amount'] # Labels
print(X.shape)
print(Y.shape)

# data Scaling
X_ = StandardScaler().fit_transform(X)
DataFrame(X_)

"""##LINEAR REGRESSION (LR)"""

from sklearn import linear_model
from plotly import figure_factory


LinearRegression1 = linear_model.SGDRegressor(random_state = 1, penalty = None) # building
Hparameter1 = {'eta0': [.0001, .001, .01, .1, 1], 'max_iter':[10000, 20000, 30000, 40000]}
grid_search1 = GridSearchCV(estimator=LinearRegression1, param_grid=Hparameter1, scoring='r2', cv=5)
grid_search1.fit(X_,Y)

# results = DataFrame.from_dict(grid_search1.cv_results_)
# print("Cross-validation results:\n", results)
best_parameters = grid_search1.best_params_
print("Best parameters: ", best_parameters)
best_result = grid_search1.best_score_
print("Best result: ", best_result)
best_model = grid_search1.best_estimator_
print("Intercept β0: ", best_model.intercept_)
print(DataFrame(zip(X.columns, best_model.coef_), columns=['Features','Coefficients']))
#print(DataFrame(zip(X.columns, best_model.coef_), columns=['Features','Coefficients']).sort_values(by=['Coefficients'],ascending=False))

"""##REGULARIZATION - L1"""

from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import GridSearchCV
from sklearn.linear_model import SGDRegressor
from plotly import figure_factory
#from sklearn.metrics import mean_squared_error, r2_score
from sklearn import linear_model
from sklearn.svm import SVR
# #  Regularization - L1
LinearRegression1 = linear_model.SGDRegressor(random_state = 1, penalty = 'l1') # model building
Hparameter2 = {'eta0': [.0001, .001, .01, .1, 1], 'max_iter':[10000, 20000, 30000, 40000],'alpha': [.001, .01, .1, 1,10, 100]}

grid_search2 = GridSearchCV(estimator=LinearRegression1, param_grid=Hparameter2, scoring='r2', cv=5)
grid_search2.fit(X, Y)

# results = DataFrame.from_dict(grid_search2.cv_results_)
# print("Cross-validation results:\n", results)
best_parameters = grid_search2.best_params_
print("Best parameters: ", best_parameters)
best_result = grid_search2.best_score_
print("Best result: ", best_result)
best_model = grid_search2.best_estimator_
print("Intercept β0: ", best_model.intercept_)
print(DataFrame(zip(X.columns, best_model.coef_), columns=['Features','Coefficients']).sort_values(by=['Coefficients'],ascending=False))

"""##  REGULARIZATION - L2"""

LinearRegression2 = linear_model.SGDRegressor(random_state = 1, penalty = 'l2') # model building
Hparameter2 = {'eta0': [.0001, .001, .01, .1, 1], 'max_iter':[10000, 20000, 30000, 40000],'alpha': [.001, .01, .1, 1,10, 100]}

grid_search2 = GridSearchCV(estimator=LinearRegression2, param_grid=Hparameter2, scoring='r2', cv=5)
grid_search2.fit(X, Y)

# results = DataFrame.from_dict(grid_search2.cv_results_)
# print("Cross-validation results:\n", results)
best_parameters = grid_search2.best_params_
print("Best parameters: ", best_parameters)
best_result = grid_search2.best_score_
print("Best result: ", best_result)
best_model = grid_search2.best_estimator_
print("Intercept β0: ", best_model.intercept_)
print(DataFrame(zip(X.columns, best_model.coef_), columns=['Features','Coefficients']).sort_values(by=['Coefficients'],ascending=False))

"""##REGULARIZATION - ELASTICNET"""

from sklearn.linear_model import SGDRegressor
LinearRegressionEN = SGDRegressor(random_state = 1, penalty = 'elasticnet') # model building
Hparameter2 = {'eta0': [.0001, .001, .01, .1, 1], 'max_iter':[10000, 20000, 30000, 40000],'alpha': [.001, .01, .1, 1,10, 100],'l1_ratio': [0.1, 0.3, 0.5, 0.7, 0.9]}

grid_search2 = GridSearchCV(estimator=LinearRegressionEN, param_grid=Hparameter2, scoring='r2', cv=5)
grid_search2.fit(X, Y)

# results = DataFrame.from_dict(grid_search2.cv_results_)
# print("Cross-validation results:\n", results)
best_parameters = grid_search2.best_params_
print("Best parameters: ", best_parameters)
best_result = grid_search2.best_score_
print("Best result: ", best_result)
best_model = grid_search2.best_estimator_
print("Intercept β0: ", best_model.intercept_)
print(DataFrame(zip(X.columns, best_model.coef_), columns=['Features','Coefficients']).sort_values(by=['Coefficients'],ascending=False))

"""##MODIFIED MEAN SQAURE ERROR (r2)"""

r, c=X_.shape
print(X_.shape)
print(r,c)
print(4/5*r,c)
modified_r2 = 1-(1-best_result)*(4/5*r-1)/(4/5*r-c-1)# 4/5*r is number of rows in training set, c is number of columns
print("modified_r2: ", modified_r2)

"""##RANDOM FOREST REGRESSION"""

from sklearn.ensemble import RandomForestRegressor
RF_Regressor1 = RandomForestRegressor(criterion='squared_error', max_features='sqrt', random_state=1)
no_Trees = {'n_estimators': [10,20,30,40,50,100]}
grid_search3 = GridSearchCV(estimator=RF_Regressor1, param_grid=no_Trees, scoring='r2', cv=5)
grid_search3.fit(X_, Y)

best_parameters = grid_search3.best_params_
print("Best parameters: ", best_parameters)
best_result = grid_search3.best_score_
print("best_score: ", best_result)
modified_r2 = 1-(1-best_result)*(4/5*r-1)/(4/5*r-c-1)
print("modified_r2: ", modified_r2)
Important_feature = Series(grid_search3.best_estimator_.feature_importances_, index=list(X)).sort_values(ascending=False) # Getting feature importances list for the best model
print(Important_feature)

# Selecting features with higher sifnificance and redefining feature set
X = data1[['Pricing', 'Number of Guests', 'Quantity of Food', 'Preparation Method']]
X_ = StandardScaler().fit_transform(X)

RF_Regressor2 = RandomForestRegressor(criterion='squared_error', max_features='sqrt', random_state=1)
no_Trees = {'n_estimators': [100,200,300,400,500]}
grid_search4 = GridSearchCV(estimator=RF_Regressor2, param_grid=no_Trees, scoring='r2', cv=5)
grid_search4.fit(X_, Y)

best_parameters = grid_search4.best_params_
print("Best parameters: ", best_parameters)
best_result = grid_search4.best_score_
print("r2: ", best_result)
modified_r2 = 1-(1-best_result)*(4/5*r-1)/(4/5*r-c-1)
print("modified_r2: ", modified_r2)
best_model = grid_search4.best_estimator_

# Feature Importance from Random Forest
# Assuming you have the RandomForestRegressor model already trained
best_rf = RandomForestRegressor(criterion='squared_error', max_features='sqrt', n_estimators=400, random_state=1)
best_rf.fit(X_, Y)

feature_importances = best_rf.feature_importances_
features = X.columns
importance_df = pd.DataFrame({'Feature': features, 'Importance': feature_importances})
importance_df = importance_df.sort_values(by='Importance', ascending=False)

plt.figure(figsize=(10, 6))
sns.barplot(x='Importance', y='Feature', data=importance_df)
plt.title('Feature Importances from Random Forest')
plt.show()

"""##Support Vector Regression"""

#Support Vector Regression
from sklearn.svm import SVR
SVRegressor = SVR()
Hparameters = {'kernel': ['linear', 'poly', 'rbf', 'sigmoid'], 'C': [10,30,60,90,110]}
grid_search1 = GridSearchCV(estimator=SVRegressor, param_grid=Hparameters, scoring='r2', cv=5)
grid_search1.fit(X_, Y)

best_parameters = grid_search1.best_params_
print("Best parameters: ", best_parameters)
best_result = grid_search1.best_score_
print("Best result: ", best_result)

"""##BAR GRAPH"""

import matplotlib.pyplot as plt
from matplotlib import pyplot as plt
import seaborn as sns
# Data Preparation
X = data1[['Pricing', 'Number of Guests', 'Quantity of Food', 'Preparation Method']]
Y = data1['Wastage Food Amount']
X_ = StandardScaler().fit_transform(X)

# Define models and their results
models = ['Linear Regression', 'L1 Regularization', 'L2 Regularization', 'ElasticNet', 'RandomForest', 'SVR']
r2_scores = [
    0.2319,  # Linear Regression
    0.2410,  # L1 Regularization
    0.2329,  # L2 Regularization
    0.2410,  # ElasticNet
    0.6641,  # RandomForest (from the final model)
    0.5273   # SVR
]
modified_r2_scores = [
    0.2305,  # Linear Regression
    0.2410,  # L1 Regularization
    0.2329,  # L2 Regularization
    0.2305,  # ElasticNet
    0.6594,  # RandomForest (from the final model)
    0.5273   # SVR
]

# Plot R2 scores and Modified R2 scores
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))

# Plot R2 scores
ax1.bar(models, r2_scores, color='red')
ax1.set_title('R2 Scores for Different Regression Models')
ax1.set_ylabel('R2 Score')
ax1.set_xticklabels(models, rotation=45, ha='right')

# Plot Modified R2 scores
ax2.bar(models, modified_r2_scores, color='blue')
ax2.set_title('Modified R2 Scores for Different Models')
ax2.set_ylabel('Modified R2 Score')
ax2.set_xticklabels(models, rotation=45, ha='right')

plt.tight_layout()
plt.show()
# Printing Reports
# For SVR, we don't have classification reports or AUC scores as it's a regression model.
# Hence, we will only show the performance metrics already computed.

# Random Forest Metrics
print("Random Forest Best Parameters: {'n_estimators': 400}")
print("Random Forest R2 Score: 0.6641")
print("Random Forest Modified R2 Score: 0.6594")

print("\nFeature Importance from Random Forest:\n", importance_df)

# For SVR metrics
print("\nSVR Best Parameters: {'C': 10, 'kernel': 'rbf'}")
print("SVR R2 Score: 0.5273")

"""##PICKLE AND PREDICTION"""

##Pickle and Prediction
joblib.dump(best_model,"model.pkl")
My_model = joblib.load("model.pkl")
x=[[1,2,3,4]]
predict= My_model.predict(x)[0]
predict=round(My_model.predict(x)[0],2)
print(predict)